#!/usr/bin/env python3
"""
JetBot Line Following v·ªõi Computer Vision
S·ª≠ d·ª•ng OpenCV ƒë·ªÉ detect ƒë∆∞·ªùng m√†u ƒëen v√† ƒëi·ªÅu khi·ªÉn robot

C√°ch ch·∫°y:
    python3            # Normalize center_x v·ªÅ [-1, 1] v√† convert to float
            normalized_x = float((center_x - width/2) / (width/2))
            confidence = float(min(area / 1000, 1.0))
            
            # T√≠nh g√≥c c·ªßa ƒë∆∞·ªùng b·∫±ng c√°ch fit line
            angle = self._calculate_line_angle(largest_contour, roi_offset, width, height)
            
            return normalized_x, confidence, angle, binaryllowing_cv.py

Nh·∫•n 'q' ƒë·ªÉ tho√°t ch∆∞∆°ng tr√¨nh
"""

import cv2
import numpy as np
import time
import threading
import sys
import signal
import json
import os
import select
from jetbot import Camera, Robot

# T·ªëi ∆∞u h√≥a NumPy operations
np.seterr(invalid='ignore', over='ignore')  # B·ªè qua warning kh√¥ng c·∫ßn thi·∫øt

# Performance monitoring (optional)
class PerformanceMonitor:
    def __init__(self):
        self.timings = {}
        self.frame_count = 0
        self.start_time = time.time()
    
    def start_timer(self, name):
        self.timings[f"{name}_start"] = time.perf_counter()
    
    def end_timer(self, name):
        if f"{name}_start" in self.timings:
            duration = time.perf_counter() - self.timings[f"{name}_start"]
            if name not in self.timings:
                self.timings[name] = []
            self.timings[name].append(duration)
            if len(self.timings[name]) > 30:  # Ch·ªâ gi·ªØ 30 samples g·∫ßn nh·∫•t
                self.timings[name].pop(0)
    
    def get_average(self, name):
        if name in self.timings and self.timings[name]:
            return sum(self.timings[name]) / len(self.timings[name])
        return 0
    
    def get_fps(self):
        self.frame_count += 1
        elapsed = time.time() - self.start_time
        return self.frame_count / elapsed if elapsed > 0 else 0

class LineFollower:
    def draw_birdview_polygon(self, image):
        """V·∫Ω c√°c ƒëi·ªÉm birdview (src points) l√™n ·∫£nh g·ªëc ƒë·ªÉ debug"""
        img = image.copy()
        h, w = img.shape[:2]
        # L·∫•y c√°c ƒëi·ªÉm t·ª´ config
        pts = [
            (int(self.src_bottom_left[0] * w), int(self.src_bottom_left[1] * h)),
            (int(self.src_bottom_right[0] * w), int(self.src_bottom_right[1] * h)),
            (int(self.src_top_right[0] * w), int(self.src_top_right[1] * h)),
            (int(self.src_top_left[0] * w), int(self.src_top_left[1] * h)),
        ]
        # V·∫Ω polygon
        cv2.polylines(img, [np.array(pts, np.int32).reshape((-1,1,2))], isClosed=True, color=(0,255,255), thickness=2)
        # V·∫Ω c√°c ƒëi·ªÉm
        for i, p in enumerate(pts):
            cv2.circle(img, p, 6, (0,0,255), -1)
            cv2.putText(img, f"P{i+1}", (p[0]+5, p[1]-5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)
        return img
    def __init__(self, config_file="config.txt"):
        print("ü§ñ Kh·ªüi t·∫°o JetBot Line Follower...")
        
        # Kh·ªüi t·∫°o camera v√† robot
        self.camera = Camera()
        self.robot = Robot()
        
        self.config_file = config_file
        
        # Th√¥ng s·ªë m·∫∑c ƒë·ªãnh (s·∫Ω ƒë∆∞·ª£c ghi ƒë√® b·ªüi config file n·∫øu c√≥)
        self.default_config = {
            # Th√¥ng s·ªë ƒëi·ªÅu khi·ªÉn
            "speed_gain": 0.15,             # T·ªëc ƒë·ªô c∆° b·∫£n (0.0-1.0)
            "steering_gain": 0.5,           # ƒê·ªô m·∫°nh steering (0.0-2.0)
            "steering_kd": 0.05,            # Derivative gain (0.0-0.5)
            "steering_bias": 0.0,           # Bias tr√°i/ph·∫£i (-0.3-0.3)
            "angle_factor": 0.3,            # Factor cho angle steering (0.0-1.0)
            
            # Th√¥ng s·ªë computer vision
            "black_threshold": 60,          # Ng∆∞·ª°ng ph√°t hi·ªán ƒëen (30-120)
            "min_area": 100,                # Di·ªán t√≠ch t·ªëi thi·ªÉu (50-500)
            "confidence_threshold": 0.1,    # Ng∆∞·ª°ng tin c·∫≠y t·ªëi thi·ªÉu (0.0-1.0)
            
            # Th√¥ng s·ªë ch·∫ø ƒë·ªô ch·∫°y
            "run": True,                     # True: ch·∫°y theo ƒë∆∞·ªùng, False: ch·ªâ align v√†o gi·ªØa
            "alignment_threshold": 0.1,      # Ng∆∞·ª°ng ƒë·ªÉ coi l√† ƒë√£ align (0.0-0.5)
            
            # Th√¥ng s·ªë Bird's Eye View
            "birdview_enabled": True,       # B·∫≠t/t·∫Øt ch·∫ø ƒë·ªô bird's eye view
            "birdview_width": 160,          # Chi·ªÅu r·ªông bird's eye view (t·ªëi ∆∞u cho JetBot)
            "birdview_height": 240,         # Chi·ªÅu cao bird's eye view (t·ªëi ∆∞u cho JetBot)
            "birdview_interpolation": "linear",  # linear, nearest, cubic
            "src_image_scale": 0.75,        # Scale down input image ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô
            
            # Th√¥ng s·ªë perspective transform (t·ª∑ l·ªá t·ª´ 0.0-1.0) - M·ªü r·ªông g√≥c nh√¨n
            "src_bottom_left": [0.05, 0.95],   # M·ªü r·ªông t·ª´ 0.15 -> 0.05 (r·ªông h∆°n)
            "src_bottom_right": [0.95, 0.95],  # M·ªü r·ªông t·ª´ 0.85 -> 0.95 (r·ªông h∆°n)
            "src_top_right": [0.65, 0.45],     # ƒê·∫©y g√≥c tr√™n l√™n cao h∆°n
            "src_top_left": [0.35, 0.45],      # ƒê·∫©y g√≥c tr√™n l√™n cao h∆°n
            "dst_margin": 0.1,                 # Gi·∫£m margin t·ª´ 0.15 -> 0.1 ƒë·ªÉ t·∫≠n d·ª•ng space
            "enable_roi_optimization": True,  # Ch·ªâ x·ª≠ l√Ω ROI quan tr·ªçng
        }
        
        # Load config t·ª´ file
        self.load_config()
        
        # Th√¥ng s·ªë Bird's Eye View - T·ªëi ∆∞u h√≥a
        self.perspective_matrix = None  # Ma tr·∫≠n perspective transform
        self.inverse_matrix = None      # Ma tr·∫≠n inverse transform
        self.interpolation_flag = cv2.INTER_LINEAR  # Interpolation method
        self.scaled_width = 640         # K√≠ch th∆∞·ªõc ·∫£nh sau scale
        self.scaled_height = 480        # K√≠ch th∆∞·ªõc ·∫£nh sau scale
        self._transform_cache = {}       # Cache cho c√°c ma tr·∫≠n transform
        
        # Bi·∫øn tr·∫°ng th√°i
        self.running = False
        self.last_steering = 0.0
        self.current_frame = None
        self.debug_frame = None
        self.birdview_frame = None
        self.is_aligned = False             # Tr·∫°ng th√°i ƒë√£ align ch∆∞a

        # Ghi video
        self.recording_enabled = True  # B·∫≠t/t·∫Øt ch·ª©c nƒÉng quay video
        self.video_writer = None
        self.video_filename = None
        self.video_fps = 20
        self.video_size = (640, 480)  # S·ª≠a n·∫øu c·∫ßn
        
        # Performance monitoring
        self.perf_monitor = PerformanceMonitor() if hasattr(self, 'enable_performance_monitoring') else None
        
        # Ph√°t hi·ªán m√¥i tr∆∞·ªùng ch·∫°y
        self.display_enabled = self._check_display_available()
        
        # Kh·ªüi t·∫°o perspective transform
        self._setup_perspective_transform()
        
        # L∆∞u ·∫£nh debug m·ªói 1s
        self.last_debug_save_time = 0
        self.debug_save_dir = "debug_images"
        if not os.path.exists(self.debug_save_dir):
            os.makedirs(self.debug_save_dir)
        print("‚úÖ Kh·ªüi t·∫°o th√†nh c√¥ng!")
        self.print_config()

    def _check_display_available(self):
        """Ki·ªÉm tra xem c√≥ th·ªÉ hi·ªÉn th·ªã GUI kh√¥ng (ph√°t hi·ªán SSH session)"""
        try:
            # Ki·ªÉm tra bi·∫øn m√¥i tr∆∞·ªùng DISPLAY
            if not os.environ.get('DISPLAY'):
                print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y DISPLAY - Ch·∫°y trong ch·∫ø ƒë·ªô SSH (kh√¥ng hi·ªÉn th·ªã)")
                return False
            
            # Ki·ªÉm tra SSH session
            if os.environ.get('SSH_CLIENT') or os.environ.get('SSH_TTY'):
                print("‚ö†Ô∏è Ph√°t hi·ªán SSH session - T·∫Øt hi·ªÉn th·ªã GUI")
                return False
            
            # Th·ª≠ t·∫°o window test
            try:
                cv2.namedWindow('test', cv2.WINDOW_AUTOSIZE)
                cv2.destroyWindow('test')
                print("‚úÖ Display kh·∫£ d·ª•ng - S·∫Ω hi·ªÉn th·ªã c·ª≠a s·ªï debug")
                return True
            except:
                print("‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫°o window - Ch·∫°y trong ch·∫ø ƒë·ªô headless")
                return False
                
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi ki·ªÉm tra display: {e} - T·∫Øt hi·ªÉn th·ªã")
            return False

    def load_config(self):
        """Load th√¥ng s·ªë t·ª´ config file"""
        try:
            if os.path.exists(self.config_file):
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    
                # Merge v·ªõi default config
                for key, value in self.default_config.items():
                    setattr(self, key, config.get(key, value))
                    
                print(f"‚úÖ ƒê√£ load config t·ª´ {self.config_file}")
            else:
                # S·ª≠ d·ª•ng default config v√† t·∫°o file m·ªõi
                for key, value in self.default_config.items():
                    setattr(self, key, value)
                    
                self.save_config()
                print(f"üìù ƒê√£ t·∫°o config m·∫∑c ƒë·ªãnh: {self.config_file}")
                
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi load config: {e}")
            print("üîÑ S·ª≠ d·ª•ng th√¥ng s·ªë m·∫∑c ƒë·ªãnh")
            
            # Fallback to default
            for key, value in self.default_config.items():
                setattr(self, key, value)

    def save_config(self):
        """L∆∞u th√¥ng s·ªë hi·ªán t·∫°i v√†o config file"""
        try:
            config = {}
            for key in self.default_config.keys():
                config[key] = getattr(self, key)
                
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=4, ensure_ascii=False)
                
            print(f"üíæ ƒê√£ l∆∞u config v√†o {self.config_file}")
            
        except Exception as e:
            print(f"‚ùå L·ªói khi l∆∞u config: {e}")
    
    def toggle_run_mode(self):
        """Toggle ch·∫ø ƒë·ªô run v√† l∆∞u v√†o config"""
        self.run = not self.run
        self.save_config()
        mode = "Follow Line" if self.run else "Alignment Only"
        print(f"üîÑ Chuy·ªÉn ƒë·ªïi ch·∫ø ƒë·ªô: {mode}")
        print(f"   Run mode: {self.run}")
        
    def console_input_loop(self):
        """X·ª≠ l√Ω input t·ª´ console trong ch·∫ø ƒë·ªô headless"""
        while self.running:
            try:
                # Ki·ªÉm tra c√≥ input trong 1 gi√¢y
                if select.select([sys.stdin], [], [], 1.0)[0]:
                    line = sys.stdin.readline().strip()
                    if line.lower() == 'r':
                        self.toggle_run_mode()
                    elif line.lower() == 'q':
                        print("üõë Ng∆∞·ªùi d√πng y√™u c·∫ßu tho√°t...")
                        self.stop()
                        break
            except:
                # Ignore input errors trong headless mode
                continue

    def print_config(self):
        """In ra th√¥ng s·ªë hi·ªán t·∫°i"""
        print(f"üìä Th√¥ng s·ªë hi·ªán t·∫°i (t·ª´ {self.config_file}):")
        print(f"   üéÆ ƒêi·ªÅu khi·ªÉn:")
        print(f"      Speed Gain: {self.speed_gain}")
        print(f"      Steering Gain: {self.steering_gain}")
        print(f"      Steering kD: {self.steering_kd}")
        print(f"      Steering Bias: {self.steering_bias}")
        print(f"      Angle Factor: {self.angle_factor}")
        print(f"   üì∑ Computer Vision:")
        print(f"      Black Threshold: {self.black_threshold}")
        print(f"      Min Area: {self.min_area}")
        print(f"      Confidence Threshold: {self.confidence_threshold}")
        print(f"   üéØ Ch·∫ø ƒë·ªô ch·∫°y:")
        print(f"      Run Mode: {'Follow Line' if self.run else 'Alignment Only'}")
        print(f"      Alignment Threshold: {self.alignment_threshold}")
        print(f"   ü¶Ö Bird's Eye View:")
        print(f"      Enabled: {'Yes' if self.birdview_enabled else 'No'}")
        print(f"      Size: {self.birdview_width}x{self.birdview_height}")
        print(f"   üì∫ Display:")
        print(f"      GUI Display: {'Enabled' if self.display_enabled else 'Disabled (SSH/Headless)'}")

    def _setup_perspective_transform(self):
        """Thi·∫øt l·∫≠p ma tr·∫≠n perspective transform t·ªëi ∆∞u cho bird's eye view"""
        # K√≠ch th∆∞·ªõc camera JetBot v·ªõi t·ªëi ∆∞u scale
        base_width, base_height = 640, 480
        self.scaled_width = int(base_width * getattr(self, 'src_image_scale', 0.75))
        self.scaled_height = int(base_height * getattr(self, 'src_image_scale', 0.75))
        
        # Thi·∫øt l·∫≠p interpolation method
        interp_method = getattr(self, 'birdview_interpolation', 'linear')
        interpolation_map = {
            'nearest': cv2.INTER_NEAREST,    # Nhanh nh·∫•t, ch·∫•t l∆∞·ª£ng th·∫•p
            'linear': cv2.INTER_LINEAR,      # C√¢n b·∫±ng t·ªëc ƒë·ªô/ch·∫•t l∆∞·ª£ng
            'cubic': cv2.INTER_CUBIC,        # Ch·∫≠m h∆°n, ch·∫•t l∆∞·ª£ng cao
            'area': cv2.INTER_AREA           # T·ªët cho downsampling
        }
        self.interpolation_flag = interpolation_map.get(interp_method, cv2.INTER_LINEAR)
        
        # ƒê·ªãnh nghƒ©a 4 ƒëi·ªÉm trong h√¨nh ·∫£nh g·ªëc (scaled) t·ª´ config
        src_points = np.float32([
            [self.scaled_width * self.src_bottom_left[0], self.scaled_height * self.src_bottom_left[1]],    
            [self.scaled_width * self.src_bottom_right[0], self.scaled_height * self.src_bottom_right[1]],  
            [self.scaled_width * self.src_top_right[0], self.scaled_height * self.src_top_right[1]],        
            [self.scaled_width * self.src_top_left[0], self.scaled_height * self.src_top_left[1]]           
        ], dtype=np.float32)  # Explicit dtype for optimization
        
        # ƒê·ªãnh nghƒ©a 4 ƒëi·ªÉm trong bird's eye view t·ª´ config
        margin = self.dst_margin
        dst_points = np.float32([
            [self.birdview_width * margin, self.birdview_height],                    
            [self.birdview_width * (1-margin), self.birdview_height],               
            [self.birdview_width * (1-margin), 0],                                  
            [self.birdview_width * margin, 0]                                       
        ], dtype=np.float32)  # Explicit dtype for optimization
        
        # T√≠nh ma tr·∫≠n perspective transform
        self.perspective_matrix = cv2.getPerspectiveTransform(src_points, dst_points)
        self.inverse_matrix = cv2.getPerspectiveTransform(dst_points, src_points)
        
        # Cache key cho optimization
        cache_key = f"{self.scaled_width}x{self.scaled_height}_{self.birdview_width}x{self.birdview_height}"
        self._transform_cache[cache_key] = {
            'perspective': self.perspective_matrix.copy(),
            'inverse': self.inverse_matrix.copy()
        }
        
        print(f"üìê Perspective transform ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a:")
        print(f"   Input size: {base_width}x{base_height} -> {self.scaled_width}x{self.scaled_height} (scale: {getattr(self, 'src_image_scale', 0.75)})")
        print(f"   Interpolation: {interp_method} ({self.interpolation_flag})")
        print(f"   Source points: {src_points.astype(int).tolist()}")
        print(f"   Bird's eye size: {self.birdview_width}x{self.birdview_height}")
        print(f"   ROI optimization: {'Enabled' if getattr(self, 'enable_roi_optimization', True) else 'Disabled'}")

    def get_birdview(self, image):
        """Chuy·ªÉn ƒë·ªïi h√¨nh ·∫£nh sang bird's eye view v·ªõi t·ªëi ∆∞u h√≥a"""
        if not self.birdview_enabled or self.perspective_matrix is None:
            return image
        
        # T·ªëi ∆∞u: Scale down input image n·∫øu c·∫ßn
        if hasattr(self, 'src_image_scale') and self.src_image_scale != 1.0:
            scaled_image = cv2.resize(
                image, 
                (self.scaled_width, self.scaled_height),
                interpolation=self.interpolation_flag
            )
        else:
            scaled_image = image
            
        # √Åp d·ª•ng perspective transform v·ªõi interpolation t·ªëi ∆∞u
        birdview = cv2.warpPerspective(
            scaled_image, 
            self.perspective_matrix, 
            (self.birdview_width, self.birdview_height),
            flags=self.interpolation_flag,
            borderMode=cv2.BORDER_CONSTANT,
            borderValue=(255, 255, 255)  # White borders
        )
        return birdview

    def detect_black_line(self, image):
        """
        Ph√°t hi·ªán ƒë∆∞·ªùng m√†u ƒëen b·∫±ng computer vision v·ªõi t·ªëi ∆∞u h√≥a
        Returns: (center_x, confidence, angle, binary) ho·∫∑c (None, 0, 0, binary)
        """
        # Chuy·ªÉn sang bird's eye view n·∫øu ƒë∆∞·ª£c b·∫≠t v·ªõi t·ªëi ∆∞u h√≥a
        if self.birdview_enabled:
            processed_image = self.get_birdview(image)
            self.birdview_frame = processed_image.copy()
        else:
            # T·ªëi ∆∞u: Scale down input image ngay c·∫£ khi kh√¥ng d√πng birdview
            if hasattr(self, 'src_image_scale') and self.src_image_scale != 1.0:
                processed_image = cv2.resize(
                    image, 
                    (self.scaled_width, self.scaled_height),
                    interpolation=self.interpolation_flag
                )
            else:
                processed_image = image
            
        # T·ªëi ∆∞u: ROI-first processing ƒë·ªÉ gi·∫£m t√≠nh to√°n
        height, width = processed_image.shape[:2]
        roi_enabled = getattr(self, 'enable_roi_optimization', True)
        
        if roi_enabled:
            # Ch·ªâ x·ª≠ l√Ω ROI quan tr·ªçng (n·ª≠a d∆∞·ªõi)
            roi_start = int(height * 0.6)  # T·ªëi ∆∞u h√≥a: 60% thay v√¨ 50%
            roi_image = processed_image[roi_start:, :].copy()  # Copy ƒë·ªÉ tr√°nh memory issues
        else:
            roi_image = processed_image
            roi_start = 0
            
        # Chuy·ªÉn sang grayscale v·ªõi memory-efficient operation
        if len(roi_image.shape) == 3:
            # T·ªëi ∆∞u: s·ª≠ d·ª•ng weighted grayscale cho accuracy cao h∆°n
            gray = cv2.cvtColor(roi_image, cv2.COLOR_BGR2GRAY)
        else:
            gray = roi_image
        
        # Blur v·ªõi kernel size t·ªëi ∆∞u cho JetBot
        kernel_size = 3 if min(gray.shape) < 200 else 5  # Adaptive kernel size
        blurred = cv2.GaussianBlur(gray, (kernel_size, kernel_size), 0)
        
        # Threshold v·ªõi adaptive method cho lighting conditions kh√°c nhau
        if hasattr(self, 'adaptive_threshold') and getattr(self, 'adaptive_threshold', False):
            binary = cv2.adaptiveThreshold(
                blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                cv2.THRESH_BINARY_INV, 11, 2
            )
        else:
            _, binary = cv2.threshold(blurred, self.black_threshold, 255, cv2.THRESH_BINARY_INV)
        
        # T√¨m contours v·ªõi t·ªëi ∆∞u h√≥a
        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if len(contours) == 0:
            return None, 0, 0, binary
        
        # T·ªëi ∆∞u: L·ªçc contours theo di·ªán t√≠ch tr∆∞·ªõc khi x·ª≠ l√Ω
        valid_contours = [c for c in contours if cv2.contourArea(c) >= self.min_area]
        
        if not valid_contours:
            return None, 0, 0, binary
        
        # T√¨m contour l·ªõn nh·∫•t trong c√°c contour h·ª£p l·ªá
        largest_contour = max(valid_contours, key=cv2.contourArea)
        area = cv2.contourArea(largest_contour)
        
        # T√≠nh center v√† angle v·ªõi vectorized operations
        M = cv2.moments(largest_contour)
        if M["m00"] != 0:
            center_x = int(M["m10"] / M["m00"])
            center_y = int(M["m01"] / M["m00"]) + roi_start
            
            # Normalize center_x v·ªÅ [-1, 1] v·ªõi float conversion t·ªëi ∆∞u
            # ƒê·∫£o ng∆∞·ª£c d·∫•u ƒë·ªÉ s·ª≠a v·∫•n ƒë·ªÅ steering ng∆∞·ª£c
            current_width = binary.shape[1]
            normalized_x = float(-(center_x - current_width/2) / (current_width/2))
            
            # Confidence calculation t·ªëi ∆∞u h√≥a
            confidence = float(min(area / 1000, 1.0))
            
            # T√≠nh g√≥c c·ªßa ƒë∆∞·ªùng b·∫±ng c√°ch fit line
            angle = self._calculate_line_angle(largest_contour, roi_start, current_width, height)
            
            return normalized_x, confidence, angle, binary
        
        return None, 0, 0, binary

    def _calculate_line_angle(self, contour, roi_offset, width, height):
        """T√≠nh g√≥c c·ªßa ƒë∆∞·ªùng t·ª´ contour v·ªõi t·ªëi ∆∞u h√≥a"""
        try:
            # T·ªëi ∆∞u: Ki·ªÉm tra k√≠ch th∆∞·ªõc contour tr∆∞·ªõc khi x·ª≠ l√Ω
            if len(contour) < 5:  # C·∫ßn √≠t nh·∫•t 5 ƒëi·ªÉm cho fitLine
                return 0.0
            
            # Fit m·ªôt ƒë∆∞·ªùng th·∫≥ng qua contour v·ªõi optimization
            [vx, vy, x, y] = cv2.fitLine(
                contour, 
                cv2.DIST_L2,    # Distance type - L2 is good balance
                0,              # Parameter (not used for L2)
                0.01,           # Radius accuracy
                0.01            # Angle accuracy
            )
            
            # T√≠nh g√≥c t·ª´ vector direction (vx, vy) - s·ª≠a l·ªói steering ng∆∞·ª£c
            # ƒê·∫£o ng∆∞·ª£c vx ƒë·ªÉ s·ª≠a v·∫•n ƒë·ªÅ qu·∫πo ng∆∞·ª£c chi·ªÅu
            angle_rad = np.arctan2(float(-vx), float(vy))  # G√≥c theo radian (ƒë·∫£o d·∫•u vx)
            angle_deg = np.degrees(angle_rad)  # Chuy·ªÉn sang ƒë·ªô
            
            # Normalize g√≥c v·ªÅ [-90, 90] ƒë·ªô - optimized  
            # FIXED: Sau khi ƒë·∫£o ng∆∞·ª£c vx, gi·ªù √¢m = r·∫Ω tr√°i, d∆∞∆°ng = r·∫Ω ph·∫£i (ƒë√∫ng)
            angle_deg = angle_deg % 360
            if angle_deg > 180:
                angle_deg -= 360
            angle_deg = np.clip(angle_deg, -90, 90)
                
            return float(angle_deg)
            
        except Exception as e:
            # Fallback gracefully
            return 0.0

    def create_debug_image(self, original, binary, center_x=None, confidence=0, steering=0, angle=0):
        """T·∫°o h√¨nh ·∫£nh debug ƒë·ªÉ hi·ªÉn th·ªã"""
        height, width = binary.shape
        
        # T·∫°o h√¨nh ·∫£nh 3 channel
        debug_img = cv2.cvtColor(binary, cv2.COLOR_GRAY2BGR)
        
        # V·∫Ω ƒë∆∞·ªùng trung t√¢m (xanh l√°)
        cv2.line(debug_img, (width//2, 0), (width//2, height), (0, 255, 0), 2)
        
        # V·∫Ω ƒëi·ªÉm center n·∫øu c√≥ (ƒë·ªè)
        if center_x is not None:
            pixel_x = int((center_x * width/2) + width/2)
            cv2.circle(debug_img, (pixel_x, height//2), 8, (0, 0, 255), -1)
            
            # V·∫Ω ƒë∆∞·ªùng steering (xanh d∆∞∆°ng)
            cv2.line(debug_img, (width//2, height//2), (pixel_x, height//2), (255, 0, 0), 3)
            
            # V·∫Ω vector h∆∞·ªõng c·ªßa ƒë∆∞·ªùng d·ª±a tr√™n angle (m√†u t√≠m)
            angle_rad = np.radians(angle)
            end_x = int(pixel_x + 30 * np.sin(angle_rad))
            end_y = int(height//2 - 30 * np.cos(angle_rad))
            cv2.arrowedLine(debug_img, (pixel_x, height//2), (end_x, end_y), (255, 0, 255), 2)
        
        # Th√™m text th√¥ng tin
        cv2.putText(debug_img, f"Confidence: {confidence:.2f}", (10, 25), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        cv2.putText(debug_img, f"Center X: {center_x:.3f}" if center_x else "No Line", 
                   (10, 45), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        cv2.putText(debug_img, f"Angle: {angle:.1f}¬∞", (10, 65), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        cv2.putText(debug_img, f"Steering: {steering:.3f}", (10, 85), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        # Th√™m th√¥ng tin v·ªÅ ch·∫ø ƒë·ªô ch·∫°y
        mode_text = "RUN" if hasattr(self, 'run') and self.run else "ALIGN"
        mode_color = (0, 255, 0) if hasattr(self, 'run') and self.run else (255, 255, 0)
        cv2.putText(debug_img, f"Mode: {mode_text}", (10, 105), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, mode_color, 1)
        
        cv2.putText(debug_img, "Press 'q' to quit", (10, height-10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 255), 1)
        
        return debug_img

    def create_combined_debug_image(self, original, debug_processed, birdview=None):
        """T·∫°o h√¨nh ·∫£nh k·∫øt h·ª£p gi·ªØa original, processed v√† bird's eye view, c√≥ v·∫Ω polygon birdview l√™n ·∫£nh g·ªëc"""
        # V·∫Ω polygon l√™n ·∫£nh g·ªëc
        original_with_poly = self.draw_birdview_polygon(original)
        if birdview is None or not self.birdview_enabled:
            # Ch·ªâ hi·ªÉn th·ªã original v√† processed
            combined = np.hstack([
                cv2.resize(original_with_poly, (300, 300)),
                cv2.resize(debug_processed, (300, 300))
            ])
        else:
            # Hi·ªÉn th·ªã c·∫£ 3: original+poly, bird's eye view, processed
            original_resized = cv2.resize(original_with_poly, (200, 200))
            birdview_resized = cv2.resize(birdview, (200, 200))
            processed_resized = cv2.resize(debug_processed, (200, 200))
            # T·∫°o layout 2x2 (ch·ªâ d√πng 3 √¥)
            top_row = np.hstack([original_resized, birdview_resized])
            bottom_row = np.hstack([processed_resized, np.zeros((200, 200, 3), dtype=np.uint8)])
            combined = np.vstack([top_row, bottom_row])
            # Th√™m label cho m·ªói view
            cv2.putText(combined, "Original+Poly", (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            cv2.putText(combined, "Bird's Eye", (210, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            cv2.putText(combined, "Processed", (10, 220), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        return combined

    def camera_callback(self, change):
        """Callback ƒë∆∞·ª£c g·ªçi khi c√≥ frame m·ªõi t·ª´ camera"""
        self.current_frame = change['new'].copy()

    def control_loop(self):
        """V√≤ng l·∫∑p ƒëi·ªÅu khi·ªÉn ch√≠nh v·ªõi performance monitoring"""
        print("üöÄ B·∫Øt ƒë·∫ßu ƒëi·ªÅu khi·ªÉn robot...")
        if self.display_enabled:
            print("üìπ Hi·ªÉn th·ªã c·ª≠a s·ªï camera - nh·∫•n 'q' ƒë·ªÉ tho√°t, 'r' ƒë·ªÉ chuy·ªÉn ch·∫ø ƒë·ªô")
        else:
            print("üñ•Ô∏è Ch·∫°y trong ch·∫ø ƒë·ªô headless - ch·ªâ c√≥ log trong terminal")
        
        frame_count = 0
        last_perf_report = time.time()
        
        # Kh·ªüi ƒë·ªông ghi video n·∫øu b·∫≠t
        if self.recording_enabled and self.video_writer is None:
            timestr = time.strftime("%Y%m%d-%H%M%S")
            self.video_filename = f"jetbot_run_{timestr}.avi"
            fourcc = cv2.VideoWriter_fourcc(*'XVID')
            self.video_writer = cv2.VideoWriter(self.video_filename, fourcc, self.video_fps, self.video_size)
            print(f"üé• ƒêang ghi video v√†o {self.video_filename}")
        
        while self.running:
            if self.current_frame is None:
                time.sleep(0.01)
                continue
                
            try:
                # Performance monitoring start
                if self.perf_monitor:
                    self.perf_monitor.start_timer('total_frame')
                    self.perf_monitor.start_timer('detect_line')
                
                # Ph√°t hi·ªán ƒë∆∞·ªùng v·ªõi bird's eye view t·ªëi ∆∞u h√≥a
                center_x, confidence, angle, binary = self.detect_black_line(self.current_frame)
                
                if self.perf_monitor:
                    self.perf_monitor.end_timer('detect_line')
                    self.perf_monitor.start_timer('control_logic')
                
                # ƒêi·ªÅu khi·ªÉn robot
                steering = 0.0
                if center_x is not None and confidence > self.confidence_threshold:
                    # Ki·ªÉm tra tr·∫°ng th√°i alignment
                    is_well_aligned = abs(center_x) < self.alignment_threshold
                    
                    if self.run or not is_well_aligned:
                        # Ch·∫ø ƒë·ªô run=True HO·∫∂C ch∆∞a align ƒë√∫ng th√¨ ti·∫øp t·ª•c ƒëi·ªÅu khi·ªÉn
                        
                        # Enhanced PID controller s·ª≠ d·ª•ng c·∫£ center_x v√† angle
                        # Steering d·ª±a tr√™n center position
                        position_steering = center_x * self.steering_gain
                        
                        # Steering d·ª±a tr√™n angle c·ªßa ƒë∆∞·ªùng (preview steering)
                        # Ch·ªâ s·ª≠ d·ª•ng angle steering khi ·ªü ch·∫ø ƒë·ªô run
                        angle_steering = 0.0
                        if self.run:
                            angle_steering = np.sin(np.radians(angle)) * self.angle_factor
                        
                        # K·∫øt h·ª£p c·∫£ hai
                        steering = (position_steering + angle_steering + 
                                   (center_x - self.last_steering) * self.steering_kd + 
                                   self.steering_bias)
                        
                        # Convert to float and limit steering
                        steering = float(max(min(steering, 1.0), -1.0))
                        self.last_steering = center_x
                        
                        # ƒêi·ªÅu khi·ªÉn ƒë·ªông c∆°
                        left_speed = max(min(self.speed_gain - steering, 1.0), 0.0)
                        right_speed = max(min(self.speed_gain + steering, 1.0), 0.0)

                        print(f"Debug: steering={steering}, left_speed={left_speed}, right_speed={right_speed}")
                        
                        # Convert to float to avoid numpy array error
                        self.robot.left_motor.value = float(left_speed)
                        self.robot.right_motor.value = float(right_speed)
                        
                        # C·∫≠p nh·∫≠t tr·∫°ng th√°i alignment
                        if not self.is_aligned and is_well_aligned and not self.run:
                            self.is_aligned = True
                            print("‚úÖ Robot ƒë√£ align v√†o gi·ªØa ƒë∆∞·ªùng - d·ª´ng l·∫°i")
                        
                        # In th√¥ng tin (m·ªói 30 frame)
                        if int(time.time() * 30) % 30 == 0:
                            mode_str = "RUN" if self.run else "ALIGN"
                            align_str = "‚úÖ" if is_well_aligned else "‚ùå"
                            print(f"üéØ [{mode_str}] Center: {center_x:.3f} {align_str}, Angle: {angle:.1f}¬∞, "
                                  f"Confidence: {confidence:.3f}, Steering: {steering:.3f}, "
                                  f"Speed: L={left_speed:.3f} R={right_speed:.3f}")
                    else:
                        # Ch·∫ø ƒë·ªô alignment v√† ƒë√£ align xong - d·ª´ng robot
                        self.robot.stop()
                        if not self.is_aligned:
                            self.is_aligned = True
                            print("‚úÖ Robot ƒë√£ align v√†o gi·ªØa ƒë∆∞·ªùng - d·ª´ng l·∫°i")
                        
                        # In th√¥ng b√°o alignment (√≠t h∆°n)
                        if int(time.time() * 5) % 30 == 0:
                            print(f"‚è∏Ô∏è [ALIGNED] Center: {center_x:.3f} ‚úÖ - Robot ƒëang d·ª´ng")
                else:
                    # Kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c ƒë∆∞·ªùng - d·ª´ng
                    self.robot.stop()
                    self.is_aligned = False  # Reset tr·∫°ng th√°i alignment
                    if int(time.time() * 10) % 30 == 0:  # In √≠t h∆°n khi kh√¥ng th·∫•y ƒë∆∞·ªùng
                        print("‚ö†Ô∏è Kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c ƒë∆∞·ªùng - Robot d·ª´ng")
                
                # T·∫°o debug image ch·ªâ khi c·∫ßn hi·ªÉn th·ªã
                if self.display_enabled:
                    debug_processed = self.create_debug_image(
                        self.current_frame, binary, center_x, confidence, steering, angle)
                    
                    # T·∫°o combined debug image v·ªõi bird's eye view
                    self.debug_frame = self.create_combined_debug_image(
                        self.current_frame, debug_processed, self.birdview_frame)
                    
                    # L∆∞u 3 ·∫£nh debug m·ªói 1s (original+poly, birdview, processed)
                    now = time.time()
                    if now - self.last_debug_save_time > 1.0:
                        try:
                            # original+poly
                            original_poly = self.draw_birdview_polygon(self.current_frame)
                            cv2.imwrite(os.path.join(self.debug_save_dir, f"original_poly_{int(now)}.jpg"), original_poly)
                            # birdview
                            if self.birdview_frame is not None:
                                cv2.imwrite(os.path.join(self.debug_save_dir, f"birdview_{int(now)}.jpg"), self.birdview_frame)
                            # processed
                            cv2.imwrite(os.path.join(self.debug_save_dir, f"processed_{int(now)}.jpg"), debug_processed)
                            self.last_debug_save_time = now
                        except Exception as e:
                            print(f"‚ö†Ô∏è L·ªói l∆∞u ·∫£nh debug: {e}")
                
                # Ghi video frame (ch·ªâ ghi frame g·ªëc, resize v·ªÅ video_size)
                if self.recording_enabled and self.video_writer is not None:
                    try:
                        frame_to_write = cv2.resize(self.current_frame, self.video_size)
                        self.video_writer.write(frame_to_write)
                    except Exception as e:
                        print(f"‚ö†Ô∏è L·ªói ghi video: {e}")
            except Exception as e:
                print(f"‚ùå L·ªói trong control loop: {e}")
                self.robot.stop()
                
            time.sleep(0.05)  # 20 FPS

    def display_loop(self):
        """V√≤ng l·∫∑p hi·ªÉn th·ªã h√¨nh ·∫£nh"""
        if not self.display_enabled:
            # Ch·∫ø ƒë·ªô headless - ch·ªâ ch·ªù t√≠n hi·ªáu d·ª´ng
            print("üñ•Ô∏è Ch·∫°y trong ch·∫ø ƒë·ªô headless - nh·∫•n Ctrl+C ƒë·ªÉ d·ª´ng")
            try:
                while self.running:
                    time.sleep(0.1)
            except KeyboardInterrupt:
                print("\nüõë Nh·∫≠n t√≠n hi·ªáu Ctrl+C...")
                self.stop()
            return
        
        # Ch·∫ø ƒë·ªô c√≥ GUI
        try:
            cv2.namedWindow('JetBot Line Following', cv2.WINDOW_AUTOSIZE)
            
            while self.running:
                if self.debug_frame is not None:
                    cv2.imshow('JetBot Line Following', self.debug_frame)
                    
                # Ki·ªÉm tra ph√≠m nh·∫•n
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q') or key == 27:  # 'q' ho·∫∑c ESC
                    print("üõë Ng∆∞·ªùi d√πng y√™u c·∫ßu tho√°t...")
                    self.stop()
                    break
                elif key == ord('r'):  # 'r' ƒë·ªÉ toggle run mode
                    self.toggle_run_mode()
                    
                time.sleep(0.03)  # ~30 FPS cho display
            
            cv2.destroyAllWindows()
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói hi·ªÉn th·ªã: {e} - Chuy·ªÉn sang ch·∫ø ƒë·ªô headless")
            # Fallback to headless mode
            try:
                while self.running:
                    time.sleep(0.1)
            except KeyboardInterrupt:
                print("\nüõë Nh·∫≠n t√≠n hi·ªáu Ctrl+C...")
                self.stop()

    def start(self):
        """B·∫Øt ƒë·∫ßu line following"""
        if self.running:
            print("‚ö†Ô∏è Robot ƒë√£ ƒëang ch·∫°y!")
            return
            
        self.running = True
        
        # B·∫Øt ƒë·∫ßu camera callback
        self.camera.observe(self.camera_callback, names='value')
        
        # T·∫°o v√† b·∫Øt ƒë·∫ßu c√°c thread
        self.control_thread = threading.Thread(target=self.control_loop, daemon=True)
        self.display_thread = threading.Thread(target=self.display_loop, daemon=True)
        
        self.control_thread.start()
        self.display_thread.start()
        
        # Th√™m console input thread cho ch·∫ø ƒë·ªô headless
        if not self.display_enabled:
            self.console_thread = threading.Thread(target=self.console_input_loop, daemon=True)
            self.console_thread.start()
        
        print("‚úÖ Line following ƒë√£ b·∫Øt ƒë·∫ßu!")
        if self.display_enabled:
            print("üí° M·∫πo: Nh·∫•n 'q' trong c·ª≠a s·ªï camera ƒë·ªÉ d·ª´ng, 'r' ƒë·ªÉ chuy·ªÉn ch·∫ø ƒë·ªô")
        else:
            print("üí° M·∫πo: Nh·∫•n Ctrl+C ƒë·ªÉ d·ª´ng robot, ho·∫∑c g√µ 'q' + Enter")
            print("üí° Trong ch·∫ø ƒë·ªô SSH: G√µ 'r' + Enter ƒë·ªÉ chuy·ªÉn ch·∫ø ƒë·ªô")
        print("üìä Theo d√µi log ƒë·ªÉ xem tr·∫°ng th√°i robot")
        
        # Ch·ªù display thread k·∫øt th√∫c (khi ng∆∞·ªùi d√πng nh·∫•n 'q')
        try:
            self.display_thread.join()
        except KeyboardInterrupt:
            print("\nüõë Nh·∫≠n t√≠n hi·ªáu Ctrl+C...")
            self.stop()

    def stop(self):
        """D·ª´ng line following"""
        if not self.running:
            return
            
        print("üõë ƒêang d·ª´ng robot...")
        self.running = False
        
        # Ng·∫Øt camera callback
        try:
            self.camera.unobserve(self.camera_callback, names='value')
        except:
            pass
        
        # D·ª´ng robot
        self.robot.stop()
        
        # Ch·ªù threads k·∫øt th√∫c
        time.sleep(0.2)
        
        print("‚úÖ Robot ƒë√£ d·ª´ng!")

    def cleanup(self):
        """Gi·∫£i ph√≥ng t√†i nguy√™n"""
        self.stop()
        try:
            self.camera.stop()
            print("üîå Camera ƒë√£ ƒë∆∞·ª£c ƒë√≥ng")
        except:
            pass

def signal_handler(sig, frame):
    """X·ª≠ l√Ω t√≠n hi·ªáu Ctrl+C"""
    print("\nüõë Nh·∫≠n t√≠n hi·ªáu d·ª´ng...")
    if 'follower' in globals():
        follower.cleanup()
    sys.exit(0)

def main():
    """H√†m main"""
    print("=" * 60)
    print("ü§ñ JETBOT LINE FOLLOWING v·ªõi COMPUTER VISION")
    print("=" * 60)
    print("üìã T√≠nh nƒÉng:")
    print("   ‚Ä¢ S·ª≠ d·ª•ng OpenCV ƒë·ªÉ detect ƒë∆∞·ªùng m√†u ƒëen")
    print("   ‚Ä¢ Bird's Eye View transformation cho g√≥c ch√≠nh x√°c")
    print("   ‚Ä¢ Enhanced PID controller v·ªõi angle prediction")
    print("   ‚Ä¢ Alignment mode: ch·ªâ cƒÉn gi·ªØa kh√¥ng ch·∫°y theo ƒë∆∞·ªùng")
    print("   ‚Ä¢ Hi·ªÉn th·ªã real-time debug window (3 views)")
    print("   ‚Ä¢ Load/Save th√¥ng s·ªë t·ª´ config.txt")
    print("   ‚Ä¢ T·ª± ƒë·ªông d·ª´ng khi kh√¥ng th·∫•y ƒë∆∞·ªùng")
    print()
    print("üéÆ ƒêi·ªÅu khi·ªÉn:")
    print("   ‚Ä¢ Desktop: Nh·∫•n 'q' trong c·ª≠a s·ªï camera ƒë·ªÉ tho√°t")
    print("   ‚Ä¢ SSH: Nh·∫•n Ctrl+C trong terminal ƒë·ªÉ tho√°t")
    print("   ‚Ä¢ Ch·ªânh s·ª≠a config.txt ƒë·ªÉ ƒëi·ªÅu ch·ªânh th√¥ng s·ªë")
    print("   ‚Ä¢ T·ª± ƒë·ªông ph√°t hi·ªán m√¥i tr∆∞·ªùng SSH/headless")
    print()
    print("‚öôÔ∏è Config:")
    print("   ‚Ä¢ run=true: ch·∫°y theo ƒë∆∞·ªùng li√™n t·ª•c")
    print("   ‚Ä¢ run=false: ch·ªâ cƒÉn gi·ªØa ƒë∆∞·ªùng r·ªìi d·ª´ng")
    print("   ‚Ä¢ alignment_threshold: ng∆∞·ª°ng ƒë·ªÉ coi l√† ƒë√£ cƒÉn gi·ªØa")
    print()
    print("üìã Y√™u c·∫ßu:")
    print("   ‚Ä¢ ƒê∆∞·ªùng m√†u ƒëen tr√™n n·ªÅn s√°ng")
    print("   ‚Ä¢ √Ånh s√°ng ƒë·ªÅu, kh√¥ng c√≥ b√≥ng ƒë·ªï")
    print("   ‚Ä¢ Robot c√≥ ƒë·ªß kh√¥ng gian an to√†n")
    print("=" * 60)
    
    # ƒêƒÉng k√Ω signal handler
    signal.signal(signal.SIGINT, signal_handler)
    
    global follower
    follower = None
    
    try:
        # T·∫°o v√† kh·ªüi ƒë·ªông line follower
        follower = LineFollower()
        
        input("\nüöÄ Nh·∫•n Enter ƒë·ªÉ b·∫Øt ƒë·∫ßu (ho·∫∑c Ctrl+C ƒë·ªÉ tho√°t)...")
        
        follower.start()
        
    except KeyboardInterrupt:
        print("\nüõë Ch∆∞∆°ng tr√¨nh b·ªã gi√°n ƒëo·∫°n")
    except Exception as e:
        print(f"‚ùå L·ªói: {e}")
    finally:
        if follower:
            follower.cleanup()

if __name__ == "__main__":
    main()